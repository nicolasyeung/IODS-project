# Clusterring and Classification

### Preparing environment
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(corrplot)

date()
```


#### Task 2 - Loading and exploring the MASS data package
```{r}
library(MASS)
data("Boston")
colnames(Boston)
dim(Boston)
str(Boston)
head(Boston)
summary(Boston)
```
##### The dataframe describes 506 towns in the Boston area across 14 variables.

```{r}
library(reshape)

Boston_melted <- melt(Boston)

ggplot(Boston_melted, aes(x = value)) +
  geom_histogram(binwidth = 1, fill = "turquoise", color = "blue") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()

```

$~$

##### Here looking at the data as a distribution we can begin to see some issues.  Variables black, crime and age seem to be skewed.  Most of the houses seem to have 6 rooms, which is quite lovely.  

$~$

#### Task 3 - Graphical overview of data and pairwise correlations 
```{r}
cor_matrix <- cor(Boston) 
corrplot(cor_matrix, method="circle")
```

$~$
##### Here I can see there are some strong negative corerelations: dis v (indus-nox-age) and mdev v lstat.  Also there are some strong positive correlations: mdev v rm (which makes sense as the more rooms you have, ideally, the higher the value of your property), idus v nox (the more industry you have the higher the relative air pollution level).

$~$

##### In looking at this data set I am really intersted, at least at first, by what crime correlates too.  Here I can see that it positively correlates to rad and tax and negatively coreelates to dis, black and mdev.  I could interpret this as crime being correlated to more rural areas and that the higher the mediam value of the properties in your town the lower the crime rate which also makes sense.

$~$

#### Task 4 - Standardize the data prior to LDA

##### Scaling the Boston data
```{r}
Boston_scaled <- as.data.frame(scale(Boston))
summary(Boston_scaled)
```

```{r}
bins <- quantile(Boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(Boston_scaled$crim, breaks = bins,labels = c("low","med_low","med_high","high"), include.lowest = TRUE)

# remove original crim from the dataset
Boston_scaled <- dplyr::select(Boston_scaled, -crim)

# add the new categorical value to scaled data
Boston_scaled <- data.frame(Boston_scaled, crime)

summary(Boston_scaled)
```
##### Just checking that the code worked and I now have a crime categorical variable and crim has been dropped.

```{r}
# Test and train data sets:
n <- nrow(Boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- Boston_scaled[ind,]

# create test set 
test <- Boston_scaled[-ind,]
```

##### Here we are preparing the data into test and train sets.  We randoming choose 80 percent of the data into df ind which was used in train and the remainder or -ind was used in test.

$~$


#### Task 5 - Fitting Linear Discriminate analysis

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
lda.fit
```


```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda (bi)plot
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

#### Task 6 - Testing the LDA for how well it could predict crime in Boston towns
```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
$~$
##### Looking at model it performs well on the high crime prediction, but the low, med_low and med_high seem to be off slightly.  med_low is the next best predicted crime level.

#### Task 7 - K means clusterring

```{r}
data("Boston")
Boston_scaled <- as.data.frame(scale(Boston))
```
##### scaling the original data 

```{r}
dist_eu <- dist(Boston_scaled, method = "euclidean")
summary(dist_eu)
```
##### measuring the euclidean distances between observations

```{r}
#dist_man <- dist(Boston_scaled, method = "manhattan")
#dist_man
```

##### using the method = "manhattan" to shoud the absolute distances between the observations

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

$~$
#####  Looking at the elbow it appears that 2 is the optimal number of clusters for k means

```{r}
# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```
##### I have no idea how to interpret this-

#### Bonus

```{r}
#Data
set.seed(32)
data("Boston")
boston_scaled <- scale(Boston) %>% as.data.frame()

#Set seven clusters:
km_bonus <- kmeans(Boston, centers = 8)
boston_scaled$cluster <- km_bonus$cluster

Boston_bonus <- lda(cluster ~ ., data = boston_scaled)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(boston_scaled$cluster)

# plot the lda (bi)plot
plot(Boston_bonus, dimen =2)
lda.arrows(Boston_bonus, myscale = 1)
```
```{r}
Boston_bonus
```

##### it seams that tax and rad have a high influence on crime followed by age

####Super Bonus

```{r}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
```
```{r}
dim(lda.fit$scaling)
```
```{r}
library(plotly)

matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers',color = train$crime)
```


